{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7cc646-3382-4281-ae0c-098d61f51e74",
   "metadata": {},
   "source": [
    "# Chapter 18. 자연어 처리\n",
    "\n",
    "이번 장에서는 순환 신경망 및 트랜스포머를 이용한 자연어처리(naturla language processing, NLP)를 다룬다.\n",
    "\n",
    "먼저 플러스의 순환 신경망 구조를 살펴보고, 문자열 생성, 텍스트 분류 등을 실습해 본다. 이어서 트랜스포머 모델 및 허깅페이스 라이브러리도 실습해본다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a2e8d-fbfb-4492-b8e4-a96b8163d9e0",
   "metadata": {},
   "source": [
    "# 18.1 순환 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8a850-ce78-441a-9890-2e59792740a1",
   "metadata": {},
   "source": [
    "- 파이토치나 텐서플로의 **순환 신경망**(recurrent neural network, RNN) 모델들은 단일 타임 스텝만 처리하는 **셀**(cell)과 입력 시퀀스를 내분의 반복문으로 한 번에 처리하는 **계층**(layer)으로 구분됨\n",
    "\n",
    "- 플럭스 역시 순환 신경망에 대해 셀과 계층을 구분한다. \n",
    "\n",
    "- 단, 플럭스의 계층은 입력 데이터의 형상에 따라 시간축(시퀀스 길이)이 있으면 셀 호출 반복을 내부적으로 처리하고, 시간축이 없으면 한 스텝만 처리하는 방식이다. \n",
    "\n",
    "줄리아는 이썬과 달리 반복문의 성능이 벡터화 이상으로 좋기 대문에 굳이 데이터셋에 시간축을 추가할 필요 없이 명시적인 반복문 안에서 모델을 호출하는 것이 선호됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dccf4-cf2f-4336-95aa-31a1d8eb637d",
   "metadata": {},
   "source": [
    "## 셀과 래퍼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4bfc0-0b94-447f-8dc3-1b78b32a87cc",
   "metadata": {},
   "source": [
    "- 셀에는 `RNNCell`, `LSTMCell`, `GRUCell`이 있고,\n",
    "\n",
    "- 각각에 대응하는 층으로 `RNN`, `LSTM`, `GRU`가 있다. \n",
    "\n",
    "가장 간단한 RNNCell 타입을 살펴보자.\n",
    "\n",
    "**(참조)** : https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b189fc-a9bb-43c0-b8c0-d266bf480dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/JULIA/chap4`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/home/bread/JULIA/chap4\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770c1259-3ed1-4c61-a48e-0fc2a1408d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431cc28a-b1df-40f0-b214-93c7a35ac2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.39472273, -0.63386273, 0.76284176], Float32[-0.39472273, -0.63386273, 0.76284176])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = Flux.RNNCell(2 => 3)\n",
    "x = rand(Float32, 2); # 입력\n",
    "h = rand(Float32, 3); # 초기 은닉 상태\n",
    "h, y = rnn(h, x)      # (다음 은닉 상태, 출력)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f98d2-5aa0-4017-a58d-7bfa3c0a1b64",
   "metadata": {},
   "source": [
    "- 층과 달리, 셀은 플럭스에서 export 되지 않으므로 Flux.RNNCell 식으로 사용했다. \n",
    "\n",
    "    - RNNCell은 in => out을 생성자의 인수로 받아서 입력 벡터에 곱해지는 (out x in) 사이즈의 가중치 Wi 필드와 은닉 상태(hidden state) 벡터에 곱해지는 (out x out) 사이즈의 가중치 Wh를 필드로 가짐\n",
    "\n",
    "    - 셀의 호출은 마지막 명령 `h, y = rnn(h, x)`와 같이, 은닉 상태와 입력값을 넘겨서 다음 은닉 상태와 출력값을 돌려 받음\n",
    "\n",
    "    - 셀 호출 시 x와 h의 각 마지막 차원에 배치 차원을 추가할 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de249ad-b2df-4ba5-8b34-cc0c39bdac4c",
   "metadata": {},
   "source": [
    "셀이 생성하는 은닉 상태를 쉽게 관리하기 위해 플럭스는 `Recur`라는 래퍼(wrapper) 타입을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3adcdf17-ff8f-4976-9010-058d6f9816ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mutable struct Recur{T, S}\n",
    "        cell::T\n",
    "        state::S\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c20899-ecc4-4442-a0fd-ae10d27cb649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function (m::Recur)(x)\n",
    "    m.state, y = m.cell(m.state, x) # 입력값 x로 객체 호출 시 내부 셀에 기존 상태와 x를 넘겨서 상태를 업데이트하고 출력값 y를 리턴함\n",
    "    return y   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c95b81-fcc2-4d12-9fa4-fdfba9990872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Functors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8876946e-daae-4a5a-8fb5-65a7fa1df28f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@functor Recur\n",
    "trainable(a::Recur) = (; cell = a.cell)   # 앞에서 다뤘던 trainable 함수로 학습 시 은닉 상태의 변경없이 셀의 파라미터만 업데이트 가능 \n",
    "reset!(m::Recur) = (m.state = m.cell.state0) # reset! 함수로 은닉 상태를 셀의 초기 상태값으로 리셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f8231-054e-48ec-8e07-c0ce7baaa2cd",
   "metadata": {},
   "source": [
    "Recur 타입은 cell과 은닉 상태 벡터인 state 필드를 가지고 있다. \n",
    "\n",
    "입력값 x로 객체 호출 시 내부 셀에 기존 상태와 x를 넘겨서 상태를 업데이트하고 출력값 y를 리턴한다.\n",
    "\n",
    "앞에서 다뤘던 trainable 함수로 학습 시 은닉 상태의 변경 없이 셀의 파라미터만 업데이트할 수 있고,\n",
    "\n",
    "reset! 함수로 은닉 상태를 셀의 초기 상태값으로 리셋할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3704b-bdaa-4492-b2b5-11388e7b7ded",
   "metadata": {},
   "source": [
    "## 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd9dd2-f5a6-4aa5-9623-c2c578efb6ea",
   "metadata": {},
   "source": [
    "RNN, LSTM, GRU는 각각 해당 셀을 래핑한 Recur 객체를 반환하는 함수\n",
    "\n",
    "플럭스 깃허브에서 가져온 RNN 함수의 소스 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b16929-ec8e-436e-affc-773482a47dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recur"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip(f, xs) = reverse([f(x) for x in reverse(xs)])\n",
    "\n",
    "function (m::Recur)(x::AbstractArray{T, 3}) where T\n",
    "  h = [m(x_t) for x_t in eachlastdim(x)]\n",
    "  sze = size(h[1])\n",
    "  reshape(reduce(hcat, h), sze[1], sze[2], length(h))\n",
    "end\n",
    "\n",
    "# Vanilla RNN\n",
    "\n",
    "struct RNNCell{F,I,H,V,S}\n",
    "  σ::F\n",
    "  Wi::I\n",
    "  Wh::H\n",
    "  b::V\n",
    "  state0::S\n",
    "end\n",
    "\n",
    "RNNCell((in, out)::Pair, σ=tanh; init=Flux.glorot_uniform, initb=zeros32, init_state=zeros32) =\n",
    "  RNNCell(σ, init(out, in), init(out, out), initb(out), init_state(out,1))\n",
    "\n",
    "function (m::RNNCell{F,I,H,V,<:AbstractMatrix{T}})(h, x::AbstractVecOrMat) where {F,I,H,V,T}\n",
    "  Wi, Wh, b = m.Wi, m.Wh, m.b\n",
    "  _size_check(m, x, 1 => size(Wi,2))\n",
    "  σ = NNlib.fast_act(m.σ, x)\n",
    "  xT = _match_eltype(m, T, x)\n",
    "  h = σ.(Wi*xT .+ Wh*h .+ b)\n",
    "  return h, reshape_cell_output(h, x)\n",
    "end\n",
    "\n",
    "@functor RNNCell\n",
    "\n",
    "function Base.show(io::IO, l::RNNCell)\n",
    "  print(io, \"RNNCell(\", size(l.Wi, 2), \" => \", size(l.Wi, 1))\n",
    "  l.σ == identity || print(io, \", \", l.σ)\n",
    "  print(io, \")\")\n",
    "end\n",
    "\n",
    "RNN(a...; ka...) = Recur(RNNCell(a...; ka...))\n",
    "Recur(m::RNNCell) = Recur(m, m.state0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a782dc-ad09-4b19-915b-4b1eab928ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recur{RNNCell{typeof(tanh), Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Matrix{Float32}}, Matrix{Float32}}(RNNCell(2 => 3, tanh), Float32[0.0; 0.0; 0.0;;])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 정의한 사용자 정의 함수로서 RNN 실행 모습\n",
    "\n",
    "m = RNN(2 => 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4911868-e022-4778-8aa4-c07e773e9b76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recur(\n",
       "  RNNCell(2 => 3, tanh),                \u001b[90m# 21 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: 4 trainable arrays, \u001b[39m21 parameters,\n",
       "\u001b[90m          # plus 1 non-trainable, 3 parameters, summarysize \u001b[39m316 bytes."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flux의 RNN 함수 사용하는 경우\n",
    "\n",
    "Flux.RNN(2 => 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292f33b-a686-48a7-9195-e1e9a884e13b",
   "metadata": {},
   "source": [
    "위 코드에서 확인되는 바와 같이, RNN은 (RNNCell, 현재 은닉 벡터)로 구성된 Recur 타입이고,\n",
    "\n",
    "Recur 타입은 입력값 x로 호출 시 은닉 상태를 업데이트하면서 출력값 y를 반환하므로 \n",
    "\n",
    "(입력 차원 x 배치 크기)를 원소로 가지는 입력 시퀀스에 대해 RNN을 반복 적용하면 다음과 같이 각 타임 스텝별 출력을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed120af3-b12d-475e-941e-24d646755936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Matrix{Float32}}:\n",
       " [-0.44558612 -0.72997475 -0.63702464 -0.42720002]\n",
       " [-0.87111557 -0.6645896 -0.38225025 -0.18847342]\n",
       " [-0.61864096 -0.57454276 -0.95116127 -1.0056882]\n",
       " [-0.496481 -0.8767482 -0.78796744 -0.8240887]\n",
       " [-0.9099736 -0.70921683 -0.62509054 -0.64954776]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 local 환경에 RNN 사용자 함수를 먼저 잡아 놓았기 때문에, Flux.RNN으로 사용\n",
    "\n",
    "using Flux\n",
    "using Random\n",
    "\n",
    "seed_value = 1\n",
    "Random.seed!(seed_value)\n",
    "\n",
    "batch_size = 4;\n",
    "seq_length = 5;\n",
    "model = Chain(Flux.RNN(2 => 3), Dense(3 => 1));\n",
    "xs = [rand(Float32, 2, batch_size) for _ = 1:seq_length]; # 행렬의 배열 xs\n",
    "[model(x) for x in xs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603d236-dc7c-48ff-a047-3ec093d7309b",
   "metadata": {},
   "source": [
    "명시적인 반복문 대신 시퀀스를 한 번에 넘기려면 시간축을 마지막 축으로 하여 (입력 차원 x 배치 크기 x 시퀀스 길이)로 넘기면 된다.\n",
    "\n",
    "다음은 시퀀스를 한 번에 넘기는 예이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46549080-90ea-4b32-9fb2-f0e931c7980e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4×5 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       " -0.445586  -0.729975  -0.637025  -0.4272\n",
       "\n",
       "[:, :, 2] =\n",
       " -0.871116  -0.66459  -0.38225  -0.188473\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.618641  -0.574543  -0.951161  -1.00569\n",
       "\n",
       "[:, :, 4] =\n",
       " -0.496481  -0.876748  -0.787967  -0.824089\n",
       "\n",
       "[:, :, 5] =\n",
       " -0.909974  -0.709217  -0.625091  -0.649548"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLUtils\n",
    "Flux.reset!(model) # 모델의 은닉 상태 벡터 초기화\n",
    "model(MLUtils.batch(xs)) # batch 함수로 행렬의 배열인 xs를 3차원 배열로 만들어 넘김"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6dc63-f323-434e-97bc-35aba0fc127c",
   "metadata": {},
   "source": [
    "모델의 은닉 상태 벡터를 초기화한 후, batch 함수로 행렬의 배열인 xs를 3차원 배열로 만들어 모델에 넘겼다.\n",
    "\n",
    "그 결과는 명시적인 반복문으로 적용한 결과와 형상만 다를 뿐 값은 같은 것을 볼 수 있다. \n",
    "\n",
    "- 시퀀스 단위의 입출력 형상은 파이토치의 RNN 계층에서 batch_first = False로 지정한 경우의 역순과 동일하다. \n",
    "\n",
    "- 시간축을 벡터화하는 대신 명시적인 반복문을 이용하면 데이터 조작에 드는 수고도 줄이고 메모리 사용량도 줄일 수 있는 경우가 많다. \n",
    "\n",
    "  - 예를 들어 슬라이딩 윈도(sliding window) 방식으로 시계열을 분석하는 경우, 시간축을 벡터화하기 위해 입력 데이터를 ((시계열 길이 - 윈도 크기 + 1) x 윈도 크기) 형태로 새로 만드는 경우가 많다. \n",
    "\n",
    "  - 반면 반복문과 view 함수를 이용하면 원 시계열 데이터를 그대로 이용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b6745-379b-4548-9218-feec05287315",
   "metadata": {},
   "source": [
    "## 손실 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdcb42-9679-4879-805e-01fe286493e2",
   "metadata": {},
   "source": [
    "각 타임 스텝에서의 출력값과 그 시점의 타깃값을 비교하는 다대다(many-to-many) 모델의 손실함수는 다음과 같이 개별 스텝에서의 손실 합으로 정의 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16f246ee-6444-4eff-ac12-d64795794952",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_fn (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_fn(loss, model, xs, ys)\n",
    "    sum(loss(model(x), y) for (x, y) in zip(xs, ys))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3cf43b-ebd3-43b1-9035-6a61a1f29014",
   "metadata": {},
   "source": [
    "반면 시퀀스의 마지막 출력값에 최종 타깃값을 비교하는 다대일(many-to-one) 모델의 손실 함수는 다음과 같이 마지막 직전 스텝까지는 은닉 상태만 업데이트하고 마지막에만 손실을 계산하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58cee02e-e4fa-4926-8eaf-3fbbc5119041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_fn (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_fn(loss, model, xs, y)\n",
    "    [m(x) for x in xs[1:end-1]]\n",
    "    loss(model(x[end]), y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b535fab-dc54-4fb0-9df1-6eb987fed0a3",
   "metadata": {},
   "source": [
    "# 18.2 문자열 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184dbb0f-4220-4487-8cbe-3c4b248d1d0d",
   "metadata": {},
   "source": [
    "이번 절에서는 문자 기반 순환 신경망을 이용하여 문자열을 생성함 \n",
    "\n",
    "주어진 문자열의 다음 문자를 예측하도록 모델을 학습시켜서, 모델이 원 텍스트와 유사하게 보이는 텍스트를 생성하는 것이 목표이다. \n",
    "\n",
    "학습 데이터 셋은 **타이니 셰익스피어 데이터셋**(Tiny Shakespeare)이고, 관련 연구는 안드레이 카파시의 블로그에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadae4be-8f30-4993-9e84-59d9e0921682",
   "metadata": {},
   "source": [
    "- (블로그 주소) https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "- (데이터 셋 파일 주소) https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b067bb5-b299-4079-9d45-4c528e02678d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux.Losses: logitcrossentropy\n",
    "import Zygote, Optimisers\n",
    "using MLUtils: chunk, batchseq\n",
    "using StatsBase: wsample\n",
    "using Formatting: printfmtln\n",
    "using Random: MersenneTwister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dda7b56-5a0b-49a1-a476-03fb39331e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/bread/JULIA/chap4\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6fbce7f-fd7d-4600-bcd4-a99952302315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = \"tinyshakespeare.txt\"\n",
    "isfile(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c7f6e-eebd-4eaf-8466-0a0fba2989b5",
   "metadata": {},
   "source": [
    "## 데이터 셋 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c595f-ea51-42cd-bace-7913e9368783",
   "metadata": {},
   "source": [
    "큰 시계열 데이터로 순환 신경망을 학습 시, 역전파를 너무 길게 하면 학습이 오래 걸리므로 적당한 시퀀스 길이로 잘라주는게 좋다.\n",
    "\n",
    "- 역전파의 연결은 시퀀스 단위로 끊어주지만, \n",
    "\n",
    "- 순전파의 경우 이번 시퀀스의 마지막 은닉 상태를 다음 시퀀스의 초기 은닉 상태로 연결하려면 시퀀스 간의 순서가 유지되어야 한다. \n",
    "\n",
    "\n",
    "미니배치 학습이 가능하면서 시퀀스 간의 순서를 유지하기 위해 다음과 같이 데이터셋을 구성한다.\n",
    "\n",
    "1. chunk 함수로 주어진 텍스트를 배치 크기로 등분한다. \n",
    "\n",
    "2. batchseq 함수로 배치 리스트를 만든다. \n",
    "\n",
    "3. chunk 함수로 배치 리스트를 시퀀스 길이 단위로 묶는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c898c2f-fb45-43a0-8068-02a73b9adf11",
   "metadata": {},
   "source": [
    "- chunk 함수는 등분되는 개수를 지정함. 두번째 chunk 함수는 size 키워드 인수로 등분되는 파트의 크기를 지정함\n",
    "\n",
    "- 먼저 전체를 배치 크기로 등분 시, 마지막 파트는 길이가 짧은 수 있고, 이 경우 batchseq 함수 적용 시 끝에 패드(pad)를 추가한다. \n",
    "\n",
    "- 세번째 단계에서 시퀀스 길이 단위로 묶을 시에도 마지막 시퀀스는 짧을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f1017-dcca-4152-b556-a36cc456b75d",
   "metadata": {},
   "source": [
    "다음은 셰익스피어 텍스트 파일을 읽어서 문자별로 원핫 인코딩을 한 후, 앞의 방식대로 입력 데이터셋과 타깃 데이터셋을 생성하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2b5ccd9-fc6d-49e2-9e19-6b5192a0d8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_data (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_data(fpath; batch_size = 32, seq_len = 100)\n",
    "    text = collect(String(read(fpath)))\n",
    "    alphabet = unique(text) # 문자 단위 모델이므로 임베딩층을 사용하지 않고 원핫 인코딩을 한다. \n",
    "    '_' in alphabet || push!(alphabet, '_'); # 문자 `'_'`는 batchseq 함수에서 필요한 패딩 값으로 사용한다.\n",
    "    \n",
    "    text = map(ch -> Flux.onehot(ch, alphabet), text)\n",
    "    pad = Flux.onehot('_', alphabet);\n",
    "    \n",
    "    Xs = batchseq(chunk(text, batch_size), pad) # 1), 2) 단계로 배치 리스트까지 만든다. \n",
    "    Xs = map(x -> hcat(x...), Xs) # 3) 단계 적용 전에 각 배치에 hcat 함수를 적용하여 원핫 벡터를 feature 수 x batch 형태의 행렬로 바꿔줌\n",
    "    Xs = chunk(Xs; size = seq_len) # 3) 단계 적용\n",
    "    \n",
    "    Ys = batchseq(chunk(text[2:end], batch_size), pad) # 타겟 데이터셋 Ys는 바로 다음 문자이므로 text[2:end]에 대해 입력 데이터셋과 동일한 방식으로 데이터셋을 만들어 줌\n",
    "    Ys = map(y -> hcat(y...), Ys)\n",
    "    Ys = chunk(Ys; size = seq_len)\n",
    "    \n",
    "    zip(Xs, Ys), alphabet \n",
    "    \n",
    "    # Xs와 Ys는 데이터 로더를 반복하여 읽는 방식으로 사용할 수 있게 zip 함수로 묶어줌\n",
    "    # 원핫 인코딩에 사용된 alphabet은 나중에 문자열 생성 시 원핫 벡터를 바꿀 때 사용하기 위해 데이터 셋과 함께 리턴함\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c01f69de-b716-4bf8-832c-cf3b014c2c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_data (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "```\n",
    "function get_data(fpath; batch_size = 32, seq_len = 100)    \n",
    "    text = collect(String(read(fpath)))\n",
    "    alphabet = unique(text)\n",
    "    '_' in alphabet || push!(alphabet, '_');\n",
    "    \n",
    "    text = map(ch -> Flux.onehot(ch, alphabet), text)\n",
    "    pad = Flux.onehot('_', alphabet);\n",
    "\n",
    "    Xs = batchseq(chunk(text, batch_size), pad)\n",
    "    Xs = map(x -> hcat(x...), Xs)\n",
    "    Xs = chunk(Xs; size = seq_len)#[1:end-1];\n",
    "\n",
    "    Ys = batchseq(chunk(text[2:end], batch_size), pad)\n",
    "    Ys = map(y -> hcat(y...), Ys)\n",
    "    Ys = chunk(Ys; size = seq_len)#[1:end-1]\n",
    "\n",
    "    zip(Xs, Ys), alphabet\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc23f82-65f2-4aba-9ee4-755bf47c0357",
   "metadata": {},
   "source": [
    "문자 단위 모델이므로 임베딩층을 사용하지 않고 원핫 인코딩을 한다. \n",
    "\n",
    "원핫 벡터 하나가 그림 18-1의 작은 칸 하나가 된다. 문자 `'_'`는 batchseq 함수에서 필요한 패딩 값으로 사용한다.\n",
    "\n",
    "1), 2) 단계로 배치 리스트까지 만든 후, 3) 단계 적용 전에 각 배치(네 칸 묶음)에 hcat 함수를 적용하여 원핫 벡터를 (features x batch) 형태의 행렬로 바꿔준다. \n",
    "\n",
    "타깃 데이터셋 Ys는 바로 다음 문자이므로 text[2:end]에 대해 입력 데이터셋과 동일한 방식으로 데이터셋을 만들어준다. \n",
    "\n",
    "Xs와 Ys는 데이터로더를 반복하여 읽는 방식으로 사용할 수 있게 zip 함수로 묶어준다. \n",
    "\n",
    "원핫 인코딩에 사용된 alphabet은 나중에 문자열 생성 시 원핫 벡터를 바꿀 때 사용하기 위해 데이터 셋과 함께 리턴한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae55988a-9412-432c-a556-f2e45f656fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader, alphabet = get_data(fpath, batch_size = 32, seq_len = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467186a-1bcb-459d-9a58-28cdb71e5e06",
   "metadata": {},
   "source": [
    "배치 크기는 32, 시퀀스 길이는 100으로 데이터셋을 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cc06945-f244-48d1-b3f6-c16a753ae1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 함수 => 16장과 동일\n",
    "\n",
    "function train(loader, model, loss_fn, optimizer)\n",
    "    num_batches = length(loader)\n",
    "    Flux.testmode!(model, false)\n",
    "    for (batch, (X, y)) in enumerate(loader)\n",
    "        X, y = Flux.gpu(X), Flux.gpu(y)\n",
    "        grad = Zygote.gradient(m -> loss_fn(m, X, y), model)[1]\n",
    "        optimizer, model = Optimisers.update(optimizer, model, grad)\n",
    "        if batch % 100 == 0\n",
    "            loss = loss_fn(model, X, y)\n",
    "            printfmtln(\"[Train] loss: {:.7f} [{:>3d}/{:>3d}]\", \n",
    "                loss, batch, num_batches)\n",
    "        end\n",
    "    end\n",
    "    model, optimizer\n",
    "end\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513ca84-19f7-46bc-bb96-811d653aa839",
   "metadata": {},
   "source": [
    "## 모델 및 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "879b0405-75f5-45f1-a4d4-a74839de80c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init(rng) = Flux.glorot_uniform(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80e0c813-bf91-44d4-aa5e-8bf904526804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function build_model(N; rng)\n",
    "    Chain(\n",
    "        LSTM(N, 512; init = init(rng)),\n",
    "        LSTM(512, 512; init = init(rng)),\n",
    "        Dense(512, N; init = init(rng)))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3716a25e-2953-4d28-b323-0775ab960cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "seed_value = 1\n",
    "Random.seed!(seed_value)\n",
    "\n",
    "rng = MersenneTwister(1);\n",
    "model = build_model(length(alphabet); rng = rng) |> gpu;\n",
    "loss_fn(m, xs, ys) = sum(logitcrossentropy.([m(x) for x in xs], ys));\n",
    "optimizer = Optimisers.setup(Optimisers.Adam(), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f05367-d54f-43a8-ba37-edc891b3a93a",
   "metadata": {},
   "source": [
    "- 모델은 원핫 벡터의 크기의 입력을 받아서 LSTM 층 두 번을 거친 후 다시 원핫 벡터 크기의 로짓을 출력함\n",
    "\n",
    "- 손실 함수는 매 스텝의 손실을 합하는 다대다 방식으로 계산\n",
    "\n",
    "- 정의한 모델과 손실함수, 옵티마이저를 이용하여 다음과 같이 20에폭을 학습\n",
    "\n",
    "    - 신경망 가중치 초기화에 사용하는 init 함수 및 학습 시 사용하는 train 함수는 16.2절 및 16.3절에서 각각 정의한 동명의 함수들을 그대로 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c973a0c6-b90f-440c-81a2-de72ad1f6368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "```\n",
    "init(rng) = Flux.glorot_uniform(rng)\n",
    "\n",
    "function build_model(N; rng)\n",
    "    Chain(\n",
    "        LSTM(N, 512; init=init(rng)),\n",
    "        LSTM(512, 512; init=init(rng)),\n",
    "        Dense(512, N; init=init(rng))\n",
    "    )\n",
    "end;\n",
    "\n",
    "rng = MersenneTwister(1)\n",
    "model = build_model(length(alphabet); rng=rng) |> gpu;\n",
    "loss_fn(m, xs, ys) = sum(logitcrossentropy.([m(x) for x in xs], ys));\n",
    "optimizer = Optimisers.setup(Optimisers.Adam(), model);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52a1b109-87c7-409e-8b98-02f67d0c17e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "[Train] loss: 207.4382935 [100/349]\n",
      "[Train] loss: 196.8038483 [200/349]\n",
      "[Train] loss: 188.2085876 [300/349]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "[Train] loss: 184.0833588 [100/349]\n",
      "[Train] loss: 176.9190979 [200/349]\n",
      "[Train] loss: 169.2527313 [300/349]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "[Train] loss: 170.0822754 [100/349]\n",
      "[Train] loss: 165.1187439 [200/349]\n",
      "[Train] loss: 158.6803894 [300/349]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "[Train] loss: 160.2009277 [100/349]\n",
      "[Train] loss: 157.0136414 [200/349]\n",
      "[Train] loss: 151.0191193 [300/349]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "[Train] loss: 153.5201874 [100/349]\n",
      "[Train] loss: 151.1076965 [200/349]\n",
      "[Train] loss: 145.3099670 [300/349]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "[Train] loss: 148.1502533 [100/349]\n",
      "[Train] loss: 146.4346924 [200/349]\n",
      "[Train] loss: 140.4650269 [300/349]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "[Train] loss: 144.6532288 [100/349]\n",
      "[Train] loss: 142.8794250 [200/349]\n",
      "[Train] loss: 136.5312958 [300/349]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "[Train] loss: 141.4434662 [100/349]\n",
      "[Train] loss: 139.8922729 [200/349]\n",
      "[Train] loss: 133.0898743 [300/349]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "[Train] loss: 138.3444366 [100/349]\n",
      "[Train] loss: 137.1343994 [200/349]\n",
      "[Train] loss: 131.2852936 [300/349]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "[Train] loss: 136.4978180 [100/349]\n",
      "[Train] loss: 134.5753174 [200/349]\n",
      "[Train] loss: 129.2937012 [300/349]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "[Train] loss: 134.4100800 [100/349]\n",
      "[Train] loss: 132.9594727 [200/349]\n",
      "[Train] loss: 127.6653214 [300/349]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "[Train] loss: 131.0516357 [100/349]\n",
      "[Train] loss: 131.3453369 [200/349]\n",
      "[Train] loss: 126.3095779 [300/349]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "[Train] loss: 129.7543488 [100/349]\n",
      "[Train] loss: 129.3619385 [200/349]\n",
      "[Train] loss: 124.2584305 [300/349]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "[Train] loss: 127.9200668 [100/349]\n",
      "[Train] loss: 127.7894363 [200/349]\n",
      "[Train] loss: 123.3396378 [300/349]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "[Train] loss: 125.9013519 [100/349]\n",
      "[Train] loss: 126.8078461 [200/349]\n",
      "[Train] loss: 121.4541931 [300/349]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "[Train] loss: 124.2856674 [100/349]\n",
      "[Train] loss: 124.2918625 [200/349]\n",
      "[Train] loss: 120.4704666 [300/349]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "[Train] loss: 122.8959885 [100/349]\n",
      "[Train] loss: 124.0814819 [200/349]\n",
      "[Train] loss: 119.3898849 [300/349]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "[Train] loss: 123.3358307 [100/349]\n",
      "[Train] loss: 122.6735458 [200/349]\n",
      "[Train] loss: 119.3738251 [300/349]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "[Train] loss: 119.7655411 [100/349]\n",
      "[Train] loss: 120.0703888 [200/349]\n",
      "[Train] loss: 117.0726242 [300/349]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "[Train] loss: 118.1575394 [100/349]\n",
      "[Train] loss: 119.0274963 [200/349]\n",
      "[Train] loss: 114.4126740 [300/349]\n",
      "711.706094 seconds (1.83 G allocations: 120.272 GiB, 4.53% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time begin\n",
    "    for epoch in 1:20\n",
    "        Flux.reset!(model)\n",
    "        println(\"Epoch $epoch\")\n",
    "        println(\"-------------------------------\")\n",
    "        global model, optimizer = train(loader, model, loss_fn, optimizer)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e737b-4b76-4f4c-b042-340e134706bc",
   "metadata": {},
   "source": [
    "- 매 에폭 시작마다 Flux.reset! 함수로 순환 신경망 모델의 은닉 상태를 초기화한다. \n",
    "\n",
    "- 한 에폭 안에서 미니배치 내 각 시퀀스 별로 시퀀스의 시작 시점에 이전 시퀀스의 마지막 은닉 상태를 이용한다. \n",
    "\n",
    "- 이러한 순환 신경망을 상태가 있는(stateful) 순환 신경망이라고 한다. \n",
    "\n",
    "- 만약 매 시퀀스 시작마다 은닉 상태를 초기화하는 상태가 없는 (stateless) 모델을 사용하려면 \n",
    "\n",
    "- train 함수 내 미니배치를 도는 반복문 안에서 Flux.reset!으로 은닉 상태를 초기화하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7900a7c5-ae7c-455c-9284-742cab32ea07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.9.3\n",
      "Commit bed2cd540a1 (2023-08-24 14:43 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: Linux (x86_64-linux-gnu)\n",
      "  CPU: 20 × 12th Gen Intel(R) Core(TM) i7-12700F\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-14.0.6 (ORCJIT, alderlake)\n",
      "  Threads: 2 on 20 virtual cores\n",
      "Environment:\n",
      "  LD_LIBRARY_PATH = :/usr/local/cuda-11.7/lib64\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98b43f-dee2-42de-be3c-fa9cb232aea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "약 12분 소요됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec571d5-00f6-4248-9c55-56761b84b921",
   "metadata": {},
   "source": [
    "## 학습 및 문자열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c044e4b9-a1f7-4ed3-802e-275ab751eb77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 모델로 가짜 셰익스피어 텍스트를 생성하는 함수를 만듦\n",
    "\n",
    "function generate(model, alphabet, init, len; rng)\n",
    "    model = model |> cpu\n",
    "    Flux.reset!(model)\n",
    "    generated = [init]\n",
    "    for _ in 1:len\n",
    "        w = softmax(model(Flux.onehot(generated[end], alphabet)))\n",
    "        push!(generated, wsample(rng, alphabet, w))\n",
    "    end\n",
    "    text = String(generated)\n",
    "    for r in split(text, '\\n')\n",
    "        println(r)\n",
    "    end\n",
    "    text\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098310a5-88f3-4cd9-a258-62205fbe88e8",
   "metadata": {},
   "source": [
    "- generate 함수는 학습된 모델과 get_data 함수에서 돌려받은 alphabet, 시작 문자, 문자열 길이를 입력받아서 해당 길이의 문자열을 생성\n",
    "\n",
    "- 주어진 문자를 원핫 벡터로 바꾸고 모델을 통과하여 나온 로짓값에 소프트맥스를 적용하면 다음 문자에 대한 확률을 얻을 수 있음\n",
    "\n",
    "- wsample 함수를 이용하여 이 확률대로 다음 문자를 샘플링하여 문자 리스트인 generated에 추가하고, 다시 이 문자를 모델에 통과시켜 그다음 문자를 샘들링한다. \n",
    "\n",
    "- 이 과정을 거쳐서 생성된 가짜 셰익스피어 텍스트의 예는 다음과 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7699e147-cd46-44ce-8f65-d13271279f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:\n",
      "Sir, Kill'd his father.\n",
      "\n",
      "Provost:\n",
      "It is prevail'd against your deal disqueste!\n",
      "What should the marched but a bitter church,\n",
      "His bust and feak?  By God of such too list another,\n",
      "And I: thy comfort is not: take my begg'd;\n",
      "So should that make a sedming formprise of;\n",
      "And that I here repose thy prisonesselve;\n",
      "And I'll tell me shall joy enough. Or how\n",
      "To she doch friends that intend of your brother's,\n",
      "Having thy libter's string with these bloody\n",
      "As only suburd my lord's choice must clamb.\n",
      "\n",
      "KING RICH\n"
     ]
    }
   ],
   "source": [
    "generate(model, alphabet, 'O', 500; rng = MersenneTwister(1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edcfb3f-8374-4e63-9eba-2ba32bf38bae",
   "metadata": {},
   "source": [
    "lstm을 이용한 문자열 생성.  끝."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
